{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dell\\miniconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from functools import wraps\n",
    "from flask import Flask\n",
    "from flask import request, Response\n",
    "from subprocess import call\n",
    "from flask import render_template\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import sklearn.ensemble as ek\n",
    "from sklearn import cross_validation, tree, linear_model\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokens(input):\n",
    "    tokens=set(re.split(r'[.-/]',input))\n",
    "    if 'com' in tokens:\n",
    "        tokens.remove('com')\n",
    "    return tokens\n",
    "\n",
    "def TL():\n",
    "    allurlscsv = pd.read_csv(\"C:\\\\Users\\\\dell\\\\Desktop\\\\Datasets\\\\URL.csv\",',',error_bad_lines=False)  #reading file\n",
    "    allurlsdata = allurlscsv.sample(frac=1).reset_index(drop=True) #converting to a dataframe\n",
    "    allurlsdata = np.array(allurlsdata)\n",
    "    random.shuffle(allurlsdata)\n",
    "    \n",
    "    y = [d[1] for d in allurlsdata]\n",
    "    #print(y)\n",
    "    corpus = [d[0] for d in allurlsdata]\n",
    "    #print(corpus)\n",
    "    vectorizer = TfidfVectorizer(tokenizer=getTokens)   #vectorizer is just an object\n",
    "    X = vectorizer.fit_transform(corpus)                #This is the method to calculate the value of tdidf values of each urls\n",
    "    #print(X)\n",
    "    model = { \"DecisionTree\":tree.DecisionTreeClassifier(max_depth=10),\n",
    "              \"RandomForest\":ek.RandomForestClassifier(n_estimators=50),\n",
    "              \"Adaboost\":ek.AdaBoostClassifier(n_estimators=50),\n",
    "              \"GradientBoosting\":ek.GradientBoostingClassifier(n_estimators=50),\n",
    "              \"LogisticRegression\":LogisticRegression()   \n",
    "    }\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  #split into training and testing set 80/20 ratio\n",
    "    results = {}\n",
    "    for algo in model:\n",
    "        clf = model[algo]\n",
    "        clf.fit(X_train,y_train)\n",
    "        score = clf.score(X_test,y_test)\n",
    "        print (\"%s : %s \" %(algo, score))\n",
    "        results[algo] = score\n",
    "    \n",
    "    winner = max(results, key=results.get)\n",
    "    print(winner)\n",
    "    return  vectorizer,model[winner]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree : 0.9182658137882018 \n",
      "RandomForest : 0.9772565742714996 \n",
      "Adaboost : 0.9147121535181236 \n",
      "GradientBoosting : 0.9104477611940298 \n",
      "LogisticRegression : 0.9445628997867804 \n",
      "RandomForest\n",
      "[1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    vectorizer, lgs  = TL()\n",
    "    X_predict = ['wikipedia.com','virus.com/search=faizanahad','pakistanifacebookforever.com/getpassword.php/','www.radsport-voggel.de/wp-admin/includes/log.exe','ahrenhei.without-transfer.ru/nethost.exe','www.itidea.it/centroesteticosothys/img/_notes/gum.exe']\n",
    "    X_predict = vectorizer.transform(X_predict)\n",
    "    y_Predict = lgs.predict(X_predict)\n",
    "    print(y_Predict)   #printing predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
